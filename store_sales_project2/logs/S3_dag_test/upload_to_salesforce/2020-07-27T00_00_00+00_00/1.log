[2020-07-29 19:18:10,993] {{taskinstance.py:616}} INFO - Dependencies all met for <TaskInstance: S3_dag_test.upload_to_salesforce 2020-07-27T00:00:00+00:00 [queued]>
[2020-07-29 19:18:11,016] {{taskinstance.py:616}} INFO - Dependencies all met for <TaskInstance: S3_dag_test.upload_to_salesforce 2020-07-27T00:00:00+00:00 [queued]>
[2020-07-29 19:18:11,017] {{taskinstance.py:834}} INFO - 
--------------------------------------------------------------------------------
[2020-07-29 19:18:11,017] {{taskinstance.py:835}} INFO - Starting attempt 1 of 1
[2020-07-29 19:18:11,017] {{taskinstance.py:836}} INFO - 
--------------------------------------------------------------------------------
[2020-07-29 19:18:11,034] {{taskinstance.py:855}} INFO - Executing <Task(PythonOperator): upload_to_salesforce> on 2020-07-27T00:00:00+00:00
[2020-07-29 19:18:11,035] {{base_task_runner.py:133}} INFO - Running: ['airflow', 'run', 'S3_dag_test', 'upload_to_salesforce', '2020-07-27T00:00:00+00:00', '--job_id', '9', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/test_s3.py', '--cfg_path', '/tmp/tmphl57gk2k']
[2020-07-29 19:18:12,403] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce [2020-07-29 19:18:12,403] {{settings.py:213}} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=2007
[2020-07-29 19:18:12,425] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce /usr/local/lib/python3.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use "pip install psycopg2-binary" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.
[2020-07-29 19:18:12,425] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce   """)
[2020-07-29 19:18:12,578] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce [2020-07-29 19:18:12,577] {{__init__.py:51}} INFO - Using executor LocalExecutor
[2020-07-29 19:18:13,332] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce [2020-07-29 19:18:13,330] {{dagbag.py:90}} INFO - Filling up the DagBag from /usr/local/airflow/dags/test_s3.py
[2020-07-29 19:18:13,349] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce /usr/local/lib/python3.7/site-packages/airflow/utils/helpers.py:425: DeprecationWarning: Importing 'DummyOperator' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
[2020-07-29 19:18:13,350] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce   DeprecationWarning)
[2020-07-29 19:18:13,351] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce /usr/local/lib/python3.7/site-packages/airflow/utils/helpers.py:425: DeprecationWarning: Importing 'PythonOperator' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
[2020-07-29 19:18:13,352] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce   DeprecationWarning)
[2020-07-29 19:18:14,178] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce [2020-07-29 19:18:14,177] {{cli.py:516}} INFO - Running <TaskInstance: S3_dag_test.upload_to_salesforce 2020-07-27T00:00:00+00:00 [running]> on host 498b75d7734d
[2020-07-29 19:18:14,226] {{python_operator.py:105}} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_ID=S3_dag_test
AIRFLOW_CTX_TASK_ID=upload_to_salesforce
AIRFLOW_CTX_EXECUTION_DATE=2020-07-27T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2020-07-27T00:00:00+00:00
[2020-07-29 19:18:14,227] {{logging_mixin.py:95}} INFO - csv file path =  /usr/local/airflow/dags
[2020-07-29 19:18:14,228] {{logging_mixin.py:95}} INFO - currrect working direc csv =  /usr/local/airflow
[2020-07-29 19:18:15,607] {{python_operator.py:114}} INFO - Done. Returned value was: None
[2020-07-29 19:18:15,942] {{logging_mixin.py:95}} INFO - [[34m2020-07-29 19:18:15,942[0m] {{[34mlocal_task_job.py:[0m105}} INFO[0m - Task exited with return code 0[0m
[2020-07-29 19:33:48,049] {{taskinstance.py:616}} INFO - Dependencies all met for <TaskInstance: S3_dag_test.upload_to_salesforce 2020-07-27T00:00:00+00:00 [queued]>
[2020-07-29 19:33:48,065] {{taskinstance.py:616}} INFO - Dependencies all met for <TaskInstance: S3_dag_test.upload_to_salesforce 2020-07-27T00:00:00+00:00 [queued]>
[2020-07-29 19:33:48,065] {{taskinstance.py:834}} INFO - 
--------------------------------------------------------------------------------
[2020-07-29 19:33:48,066] {{taskinstance.py:835}} INFO - Starting attempt 1 of 1
[2020-07-29 19:33:48,066] {{taskinstance.py:836}} INFO - 
--------------------------------------------------------------------------------
[2020-07-29 19:33:48,087] {{taskinstance.py:855}} INFO - Executing <Task(PythonOperator): upload_to_salesforce> on 2020-07-27T00:00:00+00:00
[2020-07-29 19:33:48,087] {{base_task_runner.py:133}} INFO - Running: ['airflow', 'run', 'S3_dag_test', 'upload_to_salesforce', '2020-07-27T00:00:00+00:00', '--job_id', '8', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/test_s3.py', '--cfg_path', '/tmp/tmppedd4hs8']
[2020-07-29 19:33:49,095] {{base_task_runner.py:115}} INFO - Job 8: Subtask upload_to_salesforce [2020-07-29 19:33:49,094] {{settings.py:213}} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=856
[2020-07-29 19:33:49,133] {{base_task_runner.py:115}} INFO - Job 8: Subtask upload_to_salesforce /usr/local/lib/python3.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use "pip install psycopg2-binary" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.
[2020-07-29 19:33:49,134] {{base_task_runner.py:115}} INFO - Job 8: Subtask upload_to_salesforce   """)
[2020-07-29 19:33:49,402] {{base_task_runner.py:115}} INFO - Job 8: Subtask upload_to_salesforce [2020-07-29 19:33:49,401] {{__init__.py:51}} INFO - Using executor LocalExecutor
[2020-07-29 19:33:50,168] {{base_task_runner.py:115}} INFO - Job 8: Subtask upload_to_salesforce [2020-07-29 19:33:50,168] {{dagbag.py:90}} INFO - Filling up the DagBag from /usr/local/airflow/dags/test_s3.py
[2020-07-29 19:33:50,181] {{base_task_runner.py:115}} INFO - Job 8: Subtask upload_to_salesforce /usr/local/lib/python3.7/site-packages/airflow/utils/helpers.py:425: DeprecationWarning: Importing 'DummyOperator' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
[2020-07-29 19:33:50,182] {{base_task_runner.py:115}} INFO - Job 8: Subtask upload_to_salesforce   DeprecationWarning)
[2020-07-29 19:33:50,183] {{base_task_runner.py:115}} INFO - Job 8: Subtask upload_to_salesforce /usr/local/lib/python3.7/site-packages/airflow/utils/helpers.py:425: DeprecationWarning: Importing 'PythonOperator' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
[2020-07-29 19:33:50,184] {{base_task_runner.py:115}} INFO - Job 8: Subtask upload_to_salesforce   DeprecationWarning)
[2020-07-29 19:33:50,772] {{base_task_runner.py:115}} INFO - Job 8: Subtask upload_to_salesforce [2020-07-29 19:33:50,771] {{cli.py:516}} INFO - Running <TaskInstance: S3_dag_test.upload_to_salesforce 2020-07-27T00:00:00+00:00 [running]> on host 1fdeb61556ef
[2020-07-29 19:33:50,807] {{python_operator.py:105}} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_ID=S3_dag_test
AIRFLOW_CTX_TASK_ID=upload_to_salesforce
AIRFLOW_CTX_EXECUTION_DATE=2020-07-27T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2020-07-27T00:00:00+00:00
[2020-07-29 19:33:50,807] {{logging_mixin.py:95}} INFO - csv file path =  /usr/local/airflow/dags
[2020-07-29 19:33:50,807] {{logging_mixin.py:95}} INFO - currrect working direc csv =  /usr/local/airflow
[2020-07-29 19:33:52,531] {{python_operator.py:114}} INFO - Done. Returned value was: None
[2020-07-29 19:33:53,030] {{logging_mixin.py:95}} INFO - [[34m2020-07-29 19:33:53,029[0m] {{[34mlocal_task_job.py:[0m105}} INFO[0m - Task exited with return code 0[0m
[2020-07-30 18:51:21,807] {{taskinstance.py:616}} INFO - Dependencies all met for <TaskInstance: S3_dag_test.upload_to_salesforce 2020-07-27T00:00:00+00:00 [queued]>
[2020-07-30 18:51:21,846] {{taskinstance.py:616}} INFO - Dependencies all met for <TaskInstance: S3_dag_test.upload_to_salesforce 2020-07-27T00:00:00+00:00 [queued]>
[2020-07-30 18:51:21,847] {{taskinstance.py:834}} INFO - 
--------------------------------------------------------------------------------
[2020-07-30 18:51:21,847] {{taskinstance.py:835}} INFO - Starting attempt 1 of 1
[2020-07-30 18:51:21,848] {{taskinstance.py:836}} INFO - 
--------------------------------------------------------------------------------
[2020-07-30 18:51:21,880] {{taskinstance.py:855}} INFO - Executing <Task(PythonOperator): upload_to_salesforce> on 2020-07-27T00:00:00+00:00
[2020-07-30 18:51:21,882] {{base_task_runner.py:133}} INFO - Running: ['airflow', 'run', 'S3_dag_test', 'upload_to_salesforce', '2020-07-27T00:00:00+00:00', '--job_id', '9', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/test_s3.py', '--cfg_path', '/tmp/tmpt3dgg03c']
[2020-07-30 18:51:22,873] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce [2020-07-30 18:51:22,873] {{settings.py:213}} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=5896
[2020-07-30 18:51:22,892] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce /usr/local/lib/python3.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use "pip install psycopg2-binary" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.
[2020-07-30 18:51:22,893] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce   """)
[2020-07-30 18:51:23,053] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce [2020-07-30 18:51:23,053] {{__init__.py:51}} INFO - Using executor LocalExecutor
[2020-07-30 18:51:23,934] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce [2020-07-30 18:51:23,933] {{dagbag.py:90}} INFO - Filling up the DagBag from /usr/local/airflow/dags/test_s3.py
[2020-07-30 18:51:23,948] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce /usr/local/lib/python3.7/site-packages/airflow/utils/helpers.py:425: DeprecationWarning: Importing 'DummyOperator' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
[2020-07-30 18:51:23,949] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce   DeprecationWarning)
[2020-07-30 18:51:23,951] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce /usr/local/lib/python3.7/site-packages/airflow/utils/helpers.py:425: DeprecationWarning: Importing 'PythonOperator' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
[2020-07-30 18:51:23,952] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce   DeprecationWarning)
[2020-07-30 18:51:24,598] {{base_task_runner.py:115}} INFO - Job 9: Subtask upload_to_salesforce [2020-07-30 18:51:24,598] {{cli.py:516}} INFO - Running <TaskInstance: S3_dag_test.upload_to_salesforce 2020-07-27T00:00:00+00:00 [running]> on host 6097e8989e8b
[2020-07-30 18:51:24,629] {{python_operator.py:105}} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_ID=S3_dag_test
AIRFLOW_CTX_TASK_ID=upload_to_salesforce
AIRFLOW_CTX_EXECUTION_DATE=2020-07-27T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2020-07-27T00:00:00+00:00
[2020-07-30 18:51:27,452] {{python_operator.py:114}} INFO - Done. Returned value was: None
[2020-07-30 18:51:31,738] {{logging_mixin.py:95}} INFO - [[34m2020-07-30 18:51:31,737[0m] {{[34mlocal_task_job.py:[0m105}} INFO[0m - Task exited with return code 0[0m

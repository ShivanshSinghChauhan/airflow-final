time---> [2020-08-04 21:46:29,710] filename and line ---> {{taskinstance.py:616}} level ----> INFO message ----> Dependencies all met for <TaskInstance: S3_dag_test.data_collection 2020-08-04T21:46:22.963539+00:00 [queued]>
time---> [2020-08-04 21:46:29,741] filename and line ---> {{taskinstance.py:616}} level ----> INFO message ----> Dependencies all met for <TaskInstance: S3_dag_test.data_collection 2020-08-04T21:46:22.963539+00:00 [queued]>
time---> [2020-08-04 21:46:29,742] filename and line ---> {{taskinstance.py:834}} level ----> INFO message ----> 
--------------------------------------------------------------------------------
time---> [2020-08-04 21:46:29,742] filename and line ---> {{taskinstance.py:835}} level ----> INFO message ----> Starting attempt 1 of 1
time---> [2020-08-04 21:46:29,743] filename and line ---> {{taskinstance.py:836}} level ----> INFO message ----> 
--------------------------------------------------------------------------------
time---> [2020-08-04 21:46:29,777] filename and line ---> {{taskinstance.py:855}} level ----> INFO message ----> Executing <Task(PythonOperator): data_collection> on 2020-08-04T21:46:22.963539+00:00
time---> [2020-08-04 21:46:29,779] filename and line ---> {{base_task_runner.py:133}} level ----> INFO message ----> Running: ['airflow', 'run', 'S3_dag_test', 'data_collection', '2020-08-04T21:46:22.963539+00:00', '--job_id', '3', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/test_s3.py', '--cfg_path', '/tmp/tmps0z7opb8']
time---> [2020-08-04 21:46:31,356] filename and line ---> {{base_task_runner.py:115}} level ----> INFO message ----> Job 3: Subtask data_collection [2020-08-04 21:46:31,355] {{settings.py:213}} INFO - settings.configure_orm(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=663
time---> [2020-08-04 21:46:31,379] filename and line ---> {{base_task_runner.py:115}} level ----> INFO message ----> Job 3: Subtask data_collection /usr/local/lib/python3.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use "pip install psycopg2-binary" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.
time---> [2020-08-04 21:46:31,379] filename and line ---> {{base_task_runner.py:115}} level ----> INFO message ----> Job 3: Subtask data_collection   """)
time---> [2020-08-04 21:46:31,604] filename and line ---> {{base_task_runner.py:115}} level ----> INFO message ----> Job 3: Subtask data_collection [2020-08-04 21:46:31,603] {{__init__.py:51}} INFO - Using executor LocalExecutor
time---> [2020-08-04 21:46:32,436] filename and line ---> {{base_task_runner.py:115}} level ----> INFO message ----> Job 3: Subtask data_collection [2020-08-04 21:46:32,436] {{dagbag.py:90}} INFO - Filling up the DagBag from /usr/local/airflow/dags/test_s3.py
time---> [2020-08-04 21:46:32,458] filename and line ---> {{base_task_runner.py:115}} level ----> INFO message ----> Job 3: Subtask data_collection /usr/local/lib/python3.7/site-packages/airflow/utils/helpers.py:425: DeprecationWarning: Importing 'DummyOperator' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
time---> [2020-08-04 21:46:32,462] filename and line ---> {{base_task_runner.py:115}} level ----> INFO message ----> Job 3: Subtask data_collection   DeprecationWarning)
time---> [2020-08-04 21:46:32,464] filename and line ---> {{base_task_runner.py:115}} level ----> INFO message ----> Job 3: Subtask data_collection /usr/local/lib/python3.7/site-packages/airflow/utils/helpers.py:425: DeprecationWarning: Importing 'PythonOperator' directly from 'airflow.operators' has been deprecated. Please import from 'airflow.operators.[operator_module]' instead. Support for direct imports will be dropped entirely in Airflow 2.0.
time---> [2020-08-04 21:46:32,464] filename and line ---> {{base_task_runner.py:115}} level ----> INFO message ----> Job 3: Subtask data_collection   DeprecationWarning)
time---> [2020-08-04 21:46:33,190] filename and line ---> {{base_task_runner.py:115}} level ----> INFO message ----> Job 3: Subtask data_collection [2020-08-04 21:46:33,190] {{cli.py:516}} INFO - Running <TaskInstance: S3_dag_test.data_collection 2020-08-04T21:46:22.963539+00:00 [running]> on host 9783af82c2b4
time---> [2020-08-04 21:46:33,227] filename and line ---> {{python_operator.py:105}} level ----> INFO message ----> Exporting the following env vars:
AIRFLOW_CTX_DAG_ID=S3_dag_test
AIRFLOW_CTX_TASK_ID=data_collection
AIRFLOW_CTX_EXECUTION_DATE=2020-08-04T21:46:22.963539+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2020-08-04T21:46:22.963539+00:00
time---> [2020-08-04 21:46:33,386] filename and line ---> {{python_operator.py:114}} level ----> INFO message ----> Done. Returned value was: None
time---> [2020-08-04 21:46:34,675] filename and line ---> {{logging_mixin.py:95}} level ----> INFO message ----> [[34m2020-08-04 21:46:34,675[0m] {{[34mlocal_task_job.py:[0m105}} INFO[0m - Task exited with return code 0[0m
